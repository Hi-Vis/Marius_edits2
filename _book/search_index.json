[["index.html", "Matilda Intro to R Workshop Welcome", " Matilda Intro to R Workshop Marius Mather (with tweaks by Rachel Visontay) 2022-08-26 Welcome This course has been adapted from ex-Matilda Marius Mathers R for Academics course. This session is designed to get people started with a programming approach to data management and analysis. Well be using R, but a lot of the concepts in R will transfer to other software. "],["installing.html", "Section 1 Getting started with R 1.1 Installing R 1.2 Installing RStudio 1.3 Running RStudio 1.4 Installing your first packages", " Section 1 Getting started with R 1.1 Installing R R, on its own, is basically just a command-line: you send it commands and it sends back output. It comes with a GUI (graphical user interface), but its pretty ugly and I wouldnt recommend it. To install R, go to the CRAN website and click the download link for either Windows or Mac OS X. 1.1.1 Windows instructions Click Download R for Windows Click \"base\", and download. Run the installer and install - you shouldnt need to change any of the options. 1.1.2 Mac OS X instructions Click Download R for (Mac) OS X Download the .pkg file for the latest release Open the installer and go through it: Install for all users (the default) should be fine (if you have install rights on your machine) 1.2 Installing RStudio Its best to work with R through RStudio: its a nice interface that makes it easy to save and run scripts, and see all the output and plots presented in the same window. Head to the RStudio website to download the free version of RStudio Desktop: the free version is 100% fine and isnt missing any important features. 1.2.1 Mac OS X notes RStudio offers to install some command line tools the first time you open it - go ahead and install them. You wont need them right now but they do contain some useful tools like git. 1.3 Running RStudio Open up RStudio and you should see the standard 4-pane layout: This may look like information overload at first, but most of the time, youll just be looking at the Scripts section where youve written your code. Optional: Go to Tools -&gt; Global options -&gt; Appearance and switch to a dark theme - its easier on the eyes and it looks cool. You should now be able to run your first R command by clicking in the console and typing 1 + 1 and hitting &lt;Enter&gt;: 1 + 1 ## [1] 2 1.4 Installing your first packages A lot of the most useful tools in R come from third-party packages. Thankfully theyre easy to install, through the install.packages() command. Try installing the ggplot2 package (an excellent plotting package) to make sure everything is working: install.packages(&quot;ggplot2&quot;) #install load.packages(ggplot2) #loading The install process should be automatic, and it will also install other packages that ggplot2 needs. Just in case, you should check the last few lines of the output that install.packages() produces, and look for messages like: package ggplot2 successfully unpacked and MD5 sums checked "],["tips-for-effective-r-programming.html", "Section 2 Tips for effective R programming 2.1 Working directories 2.2 Using scripts 2.3 Writing readable code 2.4 Dont panic: dealing with SPSS withdrawal", " Section 2 Tips for effective R programming Before getting into actual R code, well start with a few notes about how to use it most effectively. Bad coding habits can make your R code difficult to read and understand, so hopefully these tips will ensure you have good habits right from the start. 2.1 Working directories Wherever your working directory is, your files will save to. If you want to call in files from/save files to a different folder, you will need to specify an alternative filepath. For this reason, you should set your working directory to a convenient location at the start of each session. getwd() #to see where your current working directory is setwd() #to set your working directory For ease, we will use Rstudios point-and-click capabilities to set your working directory. In the bottom right pane, click the three dots in the top right of the pane, and navigate to the folder you would like to set as your directory. Then click More (next to the gear icon), and select Set as working directory. Having all your files in one folder and setting this as your working directory will mean the difference between having to write: data = haven::read_spss(&quot;R:/Project/2013/Analyses/Regressions/Data/Raw.sav&quot;) And: data = haven::read_spss(&quot;Data/Raw.sav&quot;) 2.2 Using scripts So far we have been typing in the console. BUT NO-ONE DOES THIS IN REAL LIFE. Its best to put every step of your data cleaning and analysis in a script that you save, rather than making temporary changes in the console. Ideally, this will mean that you (or anyone else) can run the script from top to bottom, and get the same results every time, i.e. theyre reproducible. 2.2.1 Script layout Most R scripts I write have the same basic layout: Loading the libraries Im using Loading the data Changing or analysing the data Saving the results or the recoded data file For a larger project, its good to create multiple different scripts for each stage, e.g. one file to recode the data, one to run the analyses. When saving the recoded data, its best to save it as a different file - you keep the raw data, and you can recreate the recoded data exactly by rerunning your script. R wont overwrite your data files when you change your data, unless you specifically ask it to. When you load a file into R, it lives in Rs short-term memory, and doesnt maintain any connection to the file on disk. Its only when you explicitly save to a file that those changes become permanent. ##Saving your scripts Save your script (to your current working directory) by clicking the save icon, or File&gt;Save. You will be prompted to give your script a name the first time you do this. Once saved, you will see the script appear in the bottom right pane displaying all of the files within your working directory. 2.3 Writing readable code There are two very good reasons to try to write your code in a clear, understandable way: Other people might need to use your code. You might need to use your code, a few weeks/months/years after youve written it. Its possible to write R code that works perfectly, and produces all the results and output you want, but proves very difficult to make changes to when you have to come back to it (because a reviewer asked for one additional analysis, etc.) 2.3.1 Basic formatting tips You can improve the readability of your code a lot by following a few simple rules: Put spaces between and around variable names and operators (=+-*/) Break up long lines of code Use meaningful variable names composed of 2 or 3 words (avoid abbreviations unless theyre very common and you use them very consistently) These rules can mean the difference between this: lm1=lm(y~grp+grpTime,mydf,subset=sext1==&quot;m&quot;) and this: male_difference = lm(DepressionScore ~ Group + GroupTimeInteraction, data = interview_data, subset = BaselineSex == &quot;Male&quot;) R will treat both pieces of code exactly the same, but for any humans reading, the nicer layout and meaningful names make it much easier to understand whats happening, and spot any errors in syntax or intent. 2.3.2 Keeping a consistent style Try to follow a consistent style for naming things, e.g. using snake_case for all your variable names in your R code, and TitleCase for the columns in your data. Either style is probably better than lowercase with no spacing allmashedtogether. 2.3.3 Writing comments One of the best things you can do to make R code readable and understandable is write comments - R ignores lines that start with # so you can write whatever you want and it wont affect the way your code runs. Comments that explain why something was done are great: # Need to reverse code the score for question 3 data$DepressionQ3 = 4 - data$DepressionQ3 2.4 Dont panic: dealing with SPSS withdrawal 2.4.1 RStudio has a data viewer As you get used to R, you should find that you get more comfortable using the console to check on your data. You can often see a lot of the information you need by printing the first few rows of a dataset to the console. The head() function prints the first 6 rows of a table by default, and you can select the columns that are most relevant to what youre working on if there are too many: head(iris[, c(&quot;Species&quot;, &quot;Petal.Length&quot;)]) ## Species Petal.Length ## 1 setosa 1.4 ## 2 setosa 1.4 ## 3 setosa 1.3 ## 4 setosa 1.5 ## 5 setosa 1.4 ## 6 setosa 1.7 However, you can also use RStudios built-in data viewer to get a more familiar, spreadsheet style view of your data. In the Environment pane in the top-right, you can click on the name of any data you have loaded to bring up a spreadsheet view: Data viewer example This also supports basic sorting and filtering so you can explore the data more easily (youll still need to write code using functions like arrange() or filter() if you want to actually make changes to the data though). 2.4.2 R can read SPSS files (and csvs, and almost every kind of file!) The haven package can read (and write) SPSS data files, so you can read in existing data. Place the SPSS file we sent to you in your working directory. Then run: survey_one = haven::read_spss(&quot;Personality.sav&quot;) If we click on this data in our environment in the top right pane, we can see it in the data viewer. "],["the-basics-of-r.html", "Section 3 The Basics of R 3.1 Basic data types 3.2 Converting between types 3.3 Variables: Storing Results", " Section 3 The Basics of R R is built around a few basic pieces - once you understand them, its easier to understand more complex commands, since everything is built from the same basic foundations. In programming terms, we can refer to the basic pieces that make up R as data types. 3.1 Basic data types 3.1.1 Numbers The numeric data type allows you to work with numbers. R can do all the basic operations youd expect: addition, subtraction, multiplication and division. At the most basic level, you can use R as a calculator by doing standard operations like +, -, / (division), * (multiplication), ^ (power) on numeric data: &gt; 1 + 1 ## [1] 2 &gt; 2.5 * 3 ## [1] 7.5 &gt; 8^2 ## [1] 64 R also has an integer (whole number) data type. Integers (usually) work exactly the same as numeric data, so you dont need to worry too much about the difference for now. Integers will automatically be converted to the more general numeric format when needed: # You can specify that data should be integers using &quot;L&quot; 1L + 1L ## [1] 2 # Automatically converts the result to numeric 3L + 0.1 ## [1] 3.1 5L / 2 ## [1] 2.5 3.1.2 Characters (text) The character data type allows you to store and manipulate text. Character data is created by wrapping text in either single ' or double \" quotes. In programming terms, we also refer to each chunk of text as a string: &quot;apple&quot; ## [1] &quot;apple&quot; # Note: this is still just one string. All the text, including # the spaces, is contained in the same chunk of text toupper(&quot;three bananas&quot;) ## [1] &quot;THREE BANANAS&quot; # Get part of a string. substr(&quot;carrot&quot;, 1, 3) ## [1] &quot;car&quot; # Stick multiple strings together with paste0 paste0(&quot;for&quot;, &quot;got&quot;) ## [1] &quot;forgot&quot; 3.1.3 Logical (True/False) The logical data type is used to represent the True/False result of a logical test or comparison. These are represented by the special values of TRUE and FALSE (basically 1 and 0, with special labels attached to them). To do logical comparisons, you can use syntax like: ==: equals. Note that you need a double equal sign to compare values, a single equal sign does something different. &gt; &quot;a&quot; == &quot;b&quot; ## [1] FALSE &lt;, &gt;: less than, greater than 3 &lt; 4 ## [1] TRUE &lt;=, &gt;=: less than or equal to, greater than or equal to 10 &gt;= 10 ## [1] TRUE !=: not equal to &quot;hello&quot; != &quot;world&quot; ## [1] TRUE !: not, which reverses the result of another logical test: &gt; ! (5 &gt; 3) ## [1] FALSE 3.1.3.1 Combining logicals: AND and OR More complex logical tests can be conducted by combining multiple tests with the and &amp; and or | operators. &amp; takes two logicals, e.g. a &amp; b, and returns TRUE if both a and b are TRUE, and FALSE otherwise. # Both conditions are true: TRUE &amp; TRUE is TRUE (2 &gt; 1) &amp; (&quot;a&quot; == &quot;a&quot;) ## [1] TRUE # Only one condition is true: TRUE &amp; FALSE is FALSE (3 &gt;= 3) &amp; (&quot;b&quot; == &quot;a&quot;) ## [1] FALSE a | b returns TRUE if either a or b is TRUE # FALSE | TRUE is TRUE (1 &gt; 2) | (&quot;a&quot; == &quot;a&quot;) ## [1] TRUE # FALSE | FALSE is FALSE (1 &gt; 2) | (&quot;a&quot; == &quot;b&quot;) ## [1] FALSE Its best to wrap each individual test in parentheses () to make the logic clear. 3.2 Converting between types Occasionally your data will be read in from a file as the wrong type. You might be able to fix this by changing the way you read in the file, but otherwise you should convert the data to the type that makes the most sense (you might have to clean up some invalid values first). Functions like as.character(), as.numeric() and as.logical() will convert data to the relevant type. Probably the most common type conversion youll have to do is when numeric data gets treated as text and is stored as character. Numeric operations like addition wont work until you fix this: &quot;1&quot; + 1 ## Error in &quot;1&quot; + 1: non-numeric argument to binary operator one_fixed = as.numeric(&quot;1&quot;) one_fixed + 1 ## [1] 2 3.3 Variables: Storing Results The results of calculations in R can be stored in variables: you give a name to the results, and then when you want to look at, use or change those results later, you access them using the same name. You assign a value to a variable using either = or &lt;- (these are mostly equivalent, dont worry too much about the difference), putting the variable name on the left hand side and the value on the right. NOTE THAT EVERYTHING ON THE RIGHT HAND SIDE IS BEING ASSIGNED TO THE NAME YOU GIVE ON THE LEFT scale_total = 3 + 8 + 5 + 2 + 4 # Accessing saved results scale_total ## [1] 22 # Using saved results in another calculation severe_disorder = scale_total &gt;= 15 severe_disorder ## [1] TRUE # Changing a variable: this will overwrite the old value with the # new one, the old value won&#39;t be available unless you&#39;ve # stored it somewhere else scale_total = scale_total + 2 scale_total ## [1] 24 When you assign a variable, youre asking R to remember some data so you can use it later. Understanding that simple principle will take you a long way in R programming. Variable names in R should start with a letter (a-zA-Z), and can contain letters, numbers, underscores _ and periods ., so model3, get.scores, ANX_total are all valid variable names. 3.3.1 Missing values Functions like sum() and mean() will produce a missing result by default if any values in the input are missing. Use the na.rm = TRUE option (short for NA remove) to ignore the missing values and just use the values that are available: mean(c(1, 3, NA, 7, 9)) ## [1] NA mean(c(1, 3, NA, 7, 9), na.rm = TRUE) ## [1] 5 Other functions in R will automatically remove missing values, but will usually warn you when they do. Its always good to check how missing values are being treated, whatever tool youre using. "],["dataframes-and-more.html", "Section 4 Dataframes and More 4.1 Factors (categorical data) 4.2 Dataframes!", " Section 4 Dataframes and More Once you understand the basics of Rs data types, some of the more advanced features of R start to make sense. Below, well cover some of these more advanced features. In particular, well discuss data frames, which are used to store and analyse multiple rows and columns of data bundled together in a table. 4.1 Factors (categorical data) Factors are how R represents categorical data. They have a fixed number of levels, that are set up when you first create a factor vector: severity = sample(c(&quot;Moderate&quot;, &quot;Severe&quot;), 10, replace=TRUE) # Setting &#39;levels&#39; also sets the order of the levels sev_factor = factor(severity, levels = c(&quot;Moderate&quot;, &quot;Severe&quot;)) sev_factor ## [1] Severe Moderate Severe Moderate Moderate Severe Moderate Moderate Severe Moderate ## Levels: Moderate Severe When youre testing a factor, you use the label to test it: sev_factor == &quot;Moderate&quot; ## [1] FALSE TRUE FALSE TRUE TRUE FALSE TRUE TRUE FALSE TRUE Factors can only contain data that matches their levels, and will produce a warning if you try to add something else: # Not one of the levels that was set up when the factor was # created: sev_factor[1] = &quot;Mild&quot; ## Warning in `[&lt;-.factor`(`*tmp*`, 1, value = &quot;Mild&quot;): invalid factor level, NA generated sev_factor ## [1] &lt;NA&gt; Moderate Severe Moderate Moderate Severe Moderate Moderate Severe Moderate ## Levels: Moderate Severe That said, we will convert our categorical variables in survey_one to factor survey_one$sex&lt;-as.factor(survey_one$sex) survey_one$volunteer&lt;-as.factor(survey_one$volunteer) 4.2 Dataframes! The most common format for working with data is in a table, with data arranged in rows and columns. Rs main format for tables of data is the dataframe. In a dataframe: Most of the time, youll read your data from a file (a spreadsheet, an SPSS file, etc.) and it will be read in as a dataframe. For example, survey_one is a data frame class(survey_one) ## [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; 4.2.1 Accessing parts of dataframes Accessing a single column To access a single column from a dataframe, you can use $, which will return a single vector: survey_one$sex ## [1] female male male female male male female male male male female male male male female male ## [17] male male male female male female male female female female male female female male male female ## [33] female male female male female female female female male female female male female male male female ## [49] male female male female female female female female female female female female male female male female ## [65] female female female male male male male female female female female female male female female male ## [81] male male male female female female female female female female female female male female female female ## [97] male female female male female male male male male male male female female male female female ## [113] female female male male male male male male male female male female female female female male ## [129] male female female male male female female male male male female female male female female male ## [145] male male male female female female female male male male female male female female female female ## [161] male male male female male male female male male female male female male female female male ## [177] female male female male male male female male female female male female male male male male ## [193] male male male female female male male female male female female female male female female male ## [209] male female male female male male female female male female female male female female female female ## [225] male female male male male female male male female male male male male male male male ## [241] male male female male male male male male male female male female male male male male ## [257] male female female female male male male male male male male female female female male male ## [273] male male male male female male female male female female female female female male female male ## [289] male female female female male male male male male male male male female female male female ## [305] female male female male male female male male male male male female female female male male ## [321] male female female male male female female male female male male female male female male female ## [337] male female male female female female female male female female female female male male male male ## [353] female female male female male male male male male male male female female female male male ## [369] male male female male male male male female female female female female male female male male ## [385] male female male female male female male female female male female female male male female male ## [401] female male male male male male male male female female male male female female male female ## [417] female female female male female male female male female female female female female female male male ## [433] male male female female female female male male female male female female male male female female ## [449] male male male male male female female female female female male male female male female female ## [465] male female male female female male male male male male male male male female male female ## [481] male male male male female male female female male male female male female male female male ## [497] male female female female female female female male male male female female male female male female ## [513] female male female male male male female male female female female female male female male female ## [529] male male female female female female female female male female female male female female female female ## [545] female female male female female female male female female male male female female female female female ## [561] female female female female male female female male female female female male male female male female ## [577] female female female male male female male female female female female female male female female male ## [593] female male female male female male female male female female female female male female male female ## [609] male female male female male male male male male male female male female male male female ## [625] male female female male female female male male female female male female male male male male ## [641] female female female female female female female female female female female female female female male female ## [657] female female female male male male male male female male female female female female female female ## [673] female female male female female female female female female female female female male female female female ## [689] female female female female male female female male male male female male female male female female ## [705] male male male male female male female male female female female male male female female female ## [721] male female female male male female female female female female female female female female female female ## [737] male female female female female female female female female female female female female female female female ## [753] female male female male female female female male female female female female female female female male ## [769] female female female female female female male male male male female male female male male male ## [785] male female female male male male male male female female female female female male female female ## [801] female female male male female male female female female female female female female female female female ## [817] female female female male female female female male female male female male female female female male ## [833] male male male female female female female male female female female male female male male male ## [849] male male male female female male male female male female female female female female male female ## [865] female male male female male female female female female female female female female male male male ## [881] male female male male female female female female male male male male female female female female ## [897] female female female female female female male female female female male female female female female male ## [913] female female male male male female female female female male male male male female male female ## [929] male female female male male male male male female male male female female male female female ## [945] male female female female male male male female female female female female female female female female ## [961] male male female female female female male female male female male female female male female male ## [977] female male female male male male male female male female male male female female female male ## [993] female female male male male male female male ## [ reached getOption(&quot;max.print&quot;) -- omitted 421 entries ] ## Levels: female male "],["functions.html", "Section 5 Functions 5.1 Arguments", " Section 5 Functions So far, weve used a few built-in tools like mean() - if youre coming from a language like SPSS you might think of these tools as commands, but in R we call them functions. Functions are basically reusable chunks of code, that take a certain set of inputs (also called arguments) and either produce an output (a return value), or just do a task like showing a plot. When you plug a specific set of inputs into the function and run it, we say that youre calling the function. The mean() function in R can take a vector of numbers as an input, and return a single number as an output: mean(c(5, 3, 8, 6, 3, 4)) ## [1] 4.833333 5.1 Arguments The arguments of a function are the set of inputs it accepts. Some of the inputs will be used to calculate the output, while some might be different options that affect how the calculation happens. If we look at the arguments for the default mean() function in R, accessed by entering ?mean in the console, we see: mean(x, trim = 0, na.rm = FALSE, ...) Since the first argument x appears on its own, its a mandatory argument. You have to provide a value for x, otherwise you get an error: mean() ## Error in mean.default() : argument &quot;x&quot; is missing, with no default Arguments like trim = 0 are optional when youre calling the function: the value after the = is the default value that will be used if you dont supply one. The default values tell you what types of input that argument accepts (numeric, logical, character, etc.), but its also good to read the information on the functions help page for more detail. random_scores = sample(1:50, size = 20) mean(random_scores) ## [1] 25 # This is the same as above, since this is already the default mean(random_scores, trim = 0) ## [1] 25 # A different setting from the default mean(random_scores, trim = 0.1) ## [1] 25 "],["example-data-cleaning-and-manipulation-example-descriptive-statistics.html", "Section 6 Example data cleaning and manipulation; Example descriptive statistics 6.1 Loading libraries 6.2 Loading data 6.3 Recoding 6.4 Descriptive Statistics 6.5 Analysis 6.6 Pointless flashy nonsense", " Section 6 Example data cleaning and manipulation; Example descriptive statistics Now that weve covered the fundamentals of R, we can go through an example to see everything works in practice. Well use an example dataset from the carData package, a study looking at personality and how it relates to peoples decision to volunteer as a research participant. Weve already loaded this in as survey_one Well run through some different steps, working towards some simple analyses of the data. In this format, the steps will be broken up in different chunks, but normally they would all be saved in a single script which could be run in one go. In a script, it can be useful to label each section with a commented heading - if you end a comment line with at least 4 #s, RStudio automatically treats it as a heading, and youll be able to jump to that section: ### Load libraries ##### # Code goes here ### Load data ########## # More code here First, well make sure we have the carData and sjPlot packages installed - please copy and run this code in your console (dont worry if you dont understand it for now): for (req_package in c(&quot;car&quot;, &quot;carData&quot;, &quot;sjPlot&quot;, &quot;effects&quot;)) { if (! require(req_package, character.only = TRUE)) { install.packages(req_package) } } 6.1 Loading libraries The first step in any analyses is to load any libraries were going to use. If youre part way through an analysis and realise youre going to use another library, go back and add it to the top of the script. For now, were only using the sjPlot library, which produces nice looking plots from regression models. library(sjPlot) 6.2 Loading data We already did this step using: survey_one = haven::read_spss(&quot;Personality.sav&quot;) This is a reasonably big dataset, with 1421 rows (visible in the Environment pane in RStudio, or you can get it with nrow(cow)). If you just print the data in the console by entering cow, youll get a lot of output (but luckily not all 1000+ rows). Instead, its often best to use head(data) to see the first few rows when you just want to look at the basic format of your data: head(survey_one) ## # A tibble: 6 x 4 ## neuroticism extraversion sex volunteer ## &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; ## 1 16 13 female no ## 2 8 14 male no ## 3 5 16 male no ## 4 8 20 female no ## 5 9 19 male no ## 6 6 15 male no We can check the format of the data using str(), short for structure: str(survey_one) ## tibble [1,421 x 4] (S3: tbl_df/tbl/data.frame) ## $ neuroticism : num [1:1421] 16 8 5 8 9 6 8 12 15 18 ... ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F2.0&quot; ## $ extraversion: num [1:1421] 13 14 16 20 19 15 10 11 16 7 ... ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F2.0&quot; ## $ sex : Factor w/ 2 levels &quot;female&quot;,&quot;male&quot;: 1 2 2 1 2 2 1 2 2 2 ... ## $ volunteer : Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 1 1 1 1 1 1 1 1 1 1 ... All the columns have a sensible format here: the two personality scores are integers (whole numbers), and the two categorical variables are factors1. If you need to change the type of any variables in your data, its best to do it right after loading the data, so you can work with consistent types from that point on. 6.3 Recoding Lets go through some basic recoding. First well create a variable to show whether someone is above or below the mean for extraversion. Well do this manually first, using the tools weve covered so far: # Create a vector that&#39;s all &quot;Low&quot; to start with survey_one$high_extraversion = &quot;Low&quot; # Replace the values where extraversion is high survey_one$high_extraversion[survey_one$extraversion &gt; mean(survey_one$extraversion)] = &quot;High&quot; # Make it a factor survey_one$high_extraversion = factor( survey_one$high_extraversion, levels = c(&quot;Low&quot;, &quot;High&quot;) ) Next well code people as either introverts or extroverts based on their scores. Well use a function called ifelse() to do this, which makes the process we carried out above a bit more automatic. It will also handle missing values better than our simple manual procedure, leaving missing scores missing in the result. survey_one$personality_type = ifelse( test = survey_one$extraversion &gt; survey_one$neuroticism, yes = &quot;Extravert&quot;, no = &quot;Introvert&quot; ) # Make it a factor survey_one$personality_type = factor( survey_one$personality_type, levels = c(&quot;Introvert&quot;, &quot;Extravert&quot;) ) ifelse() makes it easy to create a vector based on a test, picking values from the yes argument when the test is TRUE and from the no argument when the test is FALSE: Well also code neuroticism as either low or high based on whether its above the mean: survey_one$high_neuroticism = ifelse( survey_one$neuroticism &gt; mean(survey_one$neuroticism), &quot;High&quot;, &quot;Low&quot; ) survey_one$high_neuroticism = factor( survey_one$high_neuroticism, levels = c(&quot;Low&quot;, &quot;High&quot;) ) Since the example dataset were using doesnt have quite enough variables, lets also create a new one, a score on a depression scale similar to PHQ-9: # Advanced code: run this but don&#39;t worry too much about what # it&#39;s doing set.seed(1) survey_one$depression = round( 19 + 0.5 * survey_one$neuroticism + -0.8 * survey_one$extraversion + 0.5 * (survey_one$sex == &quot;female&quot;) + rnorm(nrow(survey_one), sd = 3) ) Well recode this depression score into categories using cut(), which allows us to divide up scores into more than two categories: survey_one$depression_diagnosis = cut( survey_one$depression, breaks = c(0, 20, 25, 33), labels = c(&quot;None&quot;, &quot;Mild&quot;, &quot;Severe&quot;), include.lowest = TRUE ) 6.4 Descriptive Statistics Before doing any actual analysis its always good to use descriptive statistics to look at the data and get a sense of what each variable looks like. 6.4.1 Quick data summary You can get a good overview of the entire dataset using summary(): summary(survey_one) ## neuroticism extraversion sex volunteer high_extraversion personality_type high_neuroticism ## Min. : 0.00 Min. : 2.00 female:780 no :824 Low :691 Introvert:663 Low :722 ## 1st Qu.: 8.00 1st Qu.:10.00 male :641 yes:597 High:730 Extravert:758 High:699 ## Median :11.00 Median :13.00 ## Mean :11.47 Mean :12.37 ## 3rd Qu.:15.00 3rd Qu.:15.00 ## Max. :24.00 Max. :23.00 ## depression depression_diagnosis ## Min. : 0.00 None :1195 ## 1st Qu.:12.00 Mild : 196 ## Median :15.00 Severe: 30 ## Mean :15.07 ## 3rd Qu.:18.00 ## Max. :33.00 6.4.2 Frequency tables You can count frequencies of categorical variables with table(): table(survey_one$sex) ## ## female male ## 780 641 table(survey_one$sex, survey_one$volunteer) ## ## no yes ## female 431 349 ## male 393 248 6.4.3 Histograms: distributions of continuous variables Histograms are good for checking the range of scores for a continuous variable to see if there are any issues like skew, outlying scores etc.. Use hist() to plot the histogram for a variable: hist(survey_one$neuroticism) 6.4.4 Scatterplots: relationship between two continuous variables Scatterplots are useful for getting a sense of whether or not theres a relationship between two continuous variables. The basic plot() function in R is quite flexible, so to produce a scatter plot we just give it the two variables and use type = 'p' to indicate we want to plot points. plot(survey_one$neuroticism, survey_one$depression, type=&#39;p&#39;) This plot doesnt look great - well cover ways to produce better plots later. But you can see theres a positive correlation between neuroticism and depression.2 To check the correlation we can use cor(): cor(survey_one$neuroticism, survey_one$depression) ## [1] 0.5341178 6.4.5 END OF WORKSHOP (continue reading for tips on analysis and more examples) 6.5 Analysis 6.5.1 T-test We can conduct a simple t test of the differences in depression scores between males and females using the t.test() function. We can get the vector of scores for males and females by subsetting and passing them to the function: dep_sex_test = t.test(survey_one$depression[survey_one$sex == &quot;male&quot;], survey_one$depression[survey_one$sex == &quot;female&quot;]) dep_sex_test ## ## Welch Two Sample t-test ## ## data: survey_one$depression[survey_one$sex == &quot;male&quot;] and survey_one$depression[survey_one$sex == &quot;female&quot;] ## t = -4.275, df = 1356.4, p-value = 2.045e-05 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -1.735880 -0.643867 ## sample estimates: ## mean of x mean of y ## 14.41654 15.60641 Since weve saved the model object (basically a list) to a variable, we can access the values were most interested in if we need to use them again: dep_sex_test$p.value ## [1] 2.044769e-05 To discover what values are available, you can look at the Value section of the ?t.test help page, or just type dep_sex_test$ and let RStudio bring up the list of suggestions. 6.5.2 Formulas: a simple mini-language for expressing models If we look at ?t.test, we can see that there are actually two different options for running a test: either pass two separate vectors of scores representing the groups we want to compare, or use a formula. Formulas in R allow you to spell out models using a compact syntax, allowing you to focus on the overall structure of your model. Formulas in R contain ~ (a tilde), with the outcome on the left of the ~ and the predictors in the model on the right. For our t-test we have depression as the outcome and sex as the grouping variable (our only predictor). Running a t test with a formula looks like: t.test(depression ~ sex, data = survey_one) ## ## Welch Two Sample t-test ## ## data: depression by sex ## t = 4.275, df = 1356.4, p-value = 2.045e-05 ## alternative hypothesis: true difference in means between group female and group male is not equal to 0 ## 95 percent confidence interval: ## 0.643867 1.735880 ## sample estimates: ## mean in group female mean in group male ## 15.60641 14.41654 When were using a formula, we can usually use a data = argument to say where all the variables in the model come from. R will automatically look them up in the dataframe, without us having to access them manually. 6.5.3 Regression We can also run a simple linear regression, prediction depression (our outcome) using neuroticism, extraversion and sex. In R, linear regression can be done using lm() (short for linear model). Our model looks like: dep_reg = lm(depression ~ neuroticism + extraversion + sex, data = survey_one) summary(dep_reg) ## ## Call: ## lm(formula = depression ~ neuroticism + extraversion + sex, data = survey_one) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9.7374 -2.0909 -0.0555 2.1794 11.8903 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 19.72960 0.36979 53.35 &lt;2e-16 *** ## neuroticism 0.49740 0.01707 29.13 &lt;2e-16 *** ## extraversion -0.82358 0.02113 -38.98 &lt;2e-16 *** ## sexmale -0.38793 0.16718 -2.32 0.0205 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.082 on 1417 degrees of freedom ## Multiple R-squared: 0.6553, Adjusted R-squared: 0.6545 ## F-statistic: 897.8 on 3 and 1417 DF, p-value: &lt; 2.2e-16 6.5.3.1 Why was that so easy? The regression model above was simple to fit because: We had all our data in a nice clean dataframe (in long format) All our continuous variables were coded as numeric All our categorical variables (sex) were coded as factors Most of the setup for running models in R happens beforehand. Factors will be treated as discrete variables, and numeric variables as continuous variables, so make sure all your variables are the right type before you try to fit a model. If you dont have variables coded the right way, go back up to the recoding section of your script and fix them there. 6.5.3.2 Formula syntax The syntax you use in formulas is special, and it doesnt necessarily mean the same thing as it would in regular R. For example + doesnt mean add these values together, it just means also include this predictor. The most important symbols in formula syntax are: +: used to separate each individual predictor time:group: : creates an interaction term between two variables, and only that interaction term. time*group: * creates an interaction term between two variables, and also includes the individual main effects (time and group in this example). Usually more useful than : because interactions generally dont make sense without the main effects. 1: when you use 1 as a predictor on its own, it means include an intercept in the model. This is the default so you dont have to include it. Intercepts make sense in most models. 0: signals that you dont want to fit an intercept. Not recommended most of the time. 6.5.3.3 Working with a fitted model Once weve saved a fitted model to a variable, we can use it, check it and save it in lots of different ways. The most useful way is to call summary(model) like we did above, which produces a summary table for the coefficients along with a few useful statistics like \\(R^2\\). There are also hundreds of different functions that people have written to work with models and provide useful output, both built in to R and available in packages. When possible, look for a function thats already been written - theres no need to reinvent the wheel. But if you want something thats not covered, all the data you need is available, and you can use it to produce exactly what you need. plot(model) (a built in command) produces a few standard diagnostic plots that do things like check the normality of your residuals and whether particular outliers are affecting the fit: # This temporarily switches R&#39;s plotting to a 2x2 layout par(mfrow = c(2, 2)) plot(dep_reg) # Switch plots back to normal par(mfrow = c(1, 1)) If you wanted to create the first plot from scratch, you could plot fitted(dep_reg) against resid(dep_reg). plot_model from the sjPlot package can give us a nice visualisation of the effects in our model, automatically choosing the right kind of plot for the predictor depending on whether its continuous or categorical: plot_model(dep_reg, # We want to see the effect of each predictor, # but lots of other plot types are available type = &quot;eff&quot;, terms = &quot;neuroticism&quot;) plot_model(dep_reg, type = &quot;eff&quot;, terms = &quot;sex&quot;) Youre also not stuck with the default presentation of results from summary(), as there are lots of ways to turn your model into a nice-looking table for publication. tab_model(), also from the sjPlot package, produces good tables for regression models: tab_model(dep_reg) Example tab_model output 6.6 Pointless flashy nonsense Impress your friends and supervisors! NOTE: Dont try to run this code (for now), it requires some libraries that are tricky to install. library(rgl) library(rayshader) hex_gg = ggplot(cow, aes(x = neuroticism, y = extraversion)) + stat_bin_hex(aes(fill = stat(density), colour = stat(density)), bins = 10, size = 1) + scale_fill_viridis_c(option = &quot;B&quot;) + scale_color_viridis_c(option = &quot;B&quot;, guide = &quot;none&quot;) + labs(x = &quot;Neuroticism&quot;, y = &quot;Extraversion&quot;, fill = &quot;&quot;, colour = &quot;&quot;) + theme_minimal() hex_gg plot_gg(hex_gg, multicore = TRUE, windowsize = c(800, 800)) render_movie(&quot;silly.mp4&quot;, phi = 40, theta = 30) Some of the most common problems in R result from text data that should just be in character format being stored as factor. factor should only be used if you have categorical variables with a fixed number of levels (usually a small number). If you have text columns, check how theyve been stored. Keen readers will notice that the positive correlation is there because we put it there when generating the fake depression variable. "],["tidyverse.html", "Section 7 Easier analysis with the tidyverse 7.1 Introduction to the tidyverse 7.2 dplyr: Turning complex analyses into simple steps", " Section 7 Easier analysis with the tidyverse Now that you know the basics of R, its time to learn that there are much better ways to do everything you just learnt! 7.1 Introduction to the tidyverse The tidyverse is a bundle of packages that make using R easier because theyre all designed to work together. Most tidy functions work well together because they: Take a dataframe as their input Return as dataframe as their output You might not use every package from the tidyverse in an analysis, but you can still load them all at the start of most analyses, and know youll have a standard set of tools available. To load them all, just use: library(tidyverse) For this session, well use the cowles data again, so lets load it up: cow = carData::Cowles 7.2 dplyr: Turning complex analyses into simple steps Artwork by @allison_horst As your analyses get more complicated, your code can get more complicated as well. To get to the answers you want, you might have to: Drop certain rows from your dataset because theyre invalid or not relevant. Calculate stats for each treatment group separately. Use summary variables like the school mean to calculate a standardized score. and more importantly, you might have to combine multiple different operations like these for each calculation youre doing. The dplyr package in the tidyverse makes this easier by providing a small number of simple verbs that can be combined easily to perform complex tasks. Each one takes your current data, changes it, and returns it. The most common verbs youll use are: filter(): choose rows to keep based on a logical test, dropping the rest. arrange(): sort the data. select(): choose columns to keep. mutate(): add new columns. summarize(): create columns that summarise the data down to a single row. count(): Count the number of rows in the data. left_join() (and right_join(), inner_join() etc.): Merge datasets based on a common identifier. And, possibly most importantly: group_by(): Split the data into groups, so that any subsequent steps happen separately for each group. Well go over examples of all of these below. 7.2.1 Bending the rules: Non-standard evaluation dplyr and other tidyverse packages bend the rules of R syntax, allowing you to just type column names like group and time rather than having to spell out the name of the dataframe each time (like survey1$group, survey1$time). Within tidyverse function calls you can just type the column name, e.g.  cow %&gt;% # Within the brackets, no $ needed count(sex, volunteer) ## sex volunteer n ## 1 female no 431 ## 2 female yes 349 ## 3 male no 393 ## 4 male yes 248 7.2.2 Pipes: %&gt;% dplyr and the tidyverse make heavy use of pipes to make it easier to carry out multiple processing steps in a row. A pipe looks like: %&gt;% If you have a calculation that takes multiple steps, it gets confusing if you run them all in one go. Here well calculate the mean of extraversion for the males in the data: summarise(filter(cow, sex == &quot;male&quot;), male_mean = mean(extraversion)) ## male_mean ## 1 12.31357 The first step here is actually filtering the data, but we have to write these function calls inside out. R understands it fine but most human beings struggle to understand the logic of whats happening, especially if theres more than 2 of these nested steps happening. You can do the steps one by one instead, but its clunky, and you might have to create multiple intermediate variables: males = filter(cow, sex == &quot;male&quot;) summarise(males, male_mean = mean(extraversion)) Using pipes, you can do each step, and then send it through the pipe to the next step. To get the mean extraversion for males using pipes, we can do: cow %&gt;% filter(sex == &quot;male&quot;) %&gt;% summarise(male_mean = mean(extraversion)) ## male_mean ## 1 12.31357 See how were not actually specifying a dataframe for the filter() and summarise() functions? Thats because the %&gt;% pipe automatically sends the data as the first argument to the next function. 7.2.3 Common tasks with dplyr Selecting columns with select You can select particular columns from a dataframe using select(). Using - in front of a column name excludes that column: # These both give the same result: inclusion vs. exclusion cow %&gt;% select(neuroticism, extraversion) %&gt;% head(2) ## neuroticism extraversion ## 1 16 13 ## 2 8 14 cow %&gt;% select(-sex, -volunteer) %&gt;% head(2) ## neuroticism extraversion ## 1 16 13 ## 2 8 14 Instead of column names, you can also use a range of helper functions like starts_with() and num_range() to select multiple columns that match a particular pattern. Creating/changing columns with mutate We can use mutate on a dataframe to add or change one or more columns: cow %&gt;% # high_extraversion and high_neuroticism are the names # of new columns that will be created mutate(high_extraversion = extraversion &gt;= 15, high_neuroticism = neuroticism &gt;= 15) %&gt;% head() ## neuroticism extraversion sex volunteer high_extraversion high_neuroticism ## 1 16 13 female no FALSE TRUE ## 2 8 14 male no FALSE FALSE ## 3 5 16 male no TRUE FALSE ## 4 8 20 female no TRUE FALSE ## 5 9 19 male no TRUE FALSE ## 6 6 15 male no TRUE FALSE If we want these changes to be saved, we would need to save the result back to the original variable. By default, mutate() will just return an altered copy of the original data, but wont change the original: # If we want to actually save the changes cow = cow %&gt;% mutate(high_extraversion = extraversion &gt;= 15, high_neuroticism = neuroticism &gt;= 15) On its own, the main advantage mutate offers is being able to spell out your calculations without including the name of the dataframe. However, it can be very useful in combination with other verbs. Summarizing the data with summarize summarize is similar to mutate: it adds or changes columns. However, summarize also collapses the data down to a single row, so the values you calculate need to be single values like means or counts. cow %&gt;% summarize( extraversion = mean(extraversion), volunteers = sum(volunteer == &quot;yes&quot;) ) ## extraversion volunteers ## 1 12.37298 597 summarize is useful in combination with group_by, where it collapses the data down to one row per group. Selecting rows with filter Choosing rows with a logical test with filter() works just like subsetting your data with a logical vector, its just easier to do it as part of a sequence of steps: cow %&gt;% filter((sex == &quot;male&quot;) &amp; (volunteer == &quot;yes&quot;)) %&gt;% head() ## neuroticism extraversion sex volunteer ## 220 17 19 male yes ## 439 7 15 male yes ## 440 17 12 male yes ## 442 6 13 male yes ## 445 8 9 male yes ## 446 5 16 male yes Sorting with arrange With arrange, you can sort by one or more columns. Use desc(column) (short for descending) to sort that column in the opposite direction: cow %&gt;% arrange(sex, volunteer, desc(extraversion)) %&gt;% head() ## neuroticism extraversion sex volunteer ## 1109 15 23 female no ## 67 15 21 female no ## 277 9 21 female no ## 875 10 21 female no ## 4 8 20 female no ## 40 5 20 female no group_by for calculations within groups group_by() is very useful for calculating different stats in subgroups of your data. This covers a lot of the more complex operations you might need to do with your data, so it unlocks a lot of possibilities. You can do things like: Mean-centering scores separately for males and females: cow %&gt;% group_by(sex) %&gt;% mutate( extraversion_centered = extraversion - mean(extraversion) ) ## # A tibble: 1,421 x 5 ## # Groups: sex [2] ## neuroticism extraversion sex volunteer extraversion_centered ## &lt;int&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 16 13 female no 0.578 ## 2 8 14 male no 1.69 ## 3 5 16 male no 3.69 ## 4 8 20 female no 7.58 ## 5 9 19 male no 6.69 ## 6 6 15 male no 2.69 ## 7 8 10 female no -2.42 ## 8 12 11 male no -1.31 ## 9 15 16 male no 3.69 ## 10 18 7 male no -5.31 ## # ... with 1,411 more rows Calculating means and SDs for subgroups: cow %&gt;% group_by(sex, volunteer) %&gt;% summarise(mean = mean(neuroticism), sd = sd(neuroticism)) ## # A tibble: 4 x 4 ## # Groups: sex [2] ## sex volunteer mean sd ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 female no 12.2 4.75 ## 2 female yes 12.3 4.79 ## 3 male no 10.5 4.75 ## 4 male yes 10.4 5.11 Frequency tables with count count() gives you a straightforward way to create a frequency table. Theres no built-in way to calculate percentages, but you can easily add them using mutate(): cow %&gt;% count(sex) %&gt;% # count creates a column called &#39;n&#39; mutate(percent = n / sum(n) * 100) ## sex n percent ## 1 female 780 54.89092 ## 2 male 641 45.10908 If you have multiple levels of grouping, you need to think about how you want to calculate percentages. To get the percentage who volunteered within each sex: cow %&gt;% count(sex, volunteer) %&gt;% group_by(sex) %&gt;% mutate(percent = n / sum(n) * 100) ## # A tibble: 4 x 4 ## # Groups: sex [2] ## sex volunteer n percent ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; ## 1 female no 431 55.3 ## 2 female yes 349 44.7 ## 3 male no 393 61.3 ## 4 male yes 248 38.7 7.2.3.1 Merging data with left_join() To combine two dataframes, you can use functions like left_join() and inner_join(). In my experience, left_join() is what you want most of the time. To merge data successfully, all you need is a column thats present in both datasets that they can be matched on, usually a participant ID or something similar. Joins can also be useful when you have to calculate a complex summary. You can create a separate table with the summary info, and merge it back into the main dataset. As a simple example, lets summarize extraversion by both sex and volunteering status and merge it back into the main dataset: extraversion_info = cow %&gt;% group_by(sex, volunteer) %&gt;% summarize(mean_extraversion = mean(extraversion)) extraversion_info ## # A tibble: 4 x 3 ## # Groups: sex [2] ## sex volunteer mean_extraversion ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 female no 12.0 ## 2 female yes 12.9 ## 3 male no 11.9 ## 4 male yes 12.9 cow %&gt;% left_join(extraversion_info, by = c(&quot;sex&quot;, &quot;volunteer&quot;)) %&gt;% head() ## neuroticism extraversion sex volunteer mean_extraversion ## 1 16 13 female no 12.00696 ## 2 8 14 male no 11.91349 ## 3 5 16 male no 11.91349 ## 4 8 20 female no 12.00696 ## 5 9 19 male no 11.91349 ## 6 6 15 male no 11.91349 "],["better-plots-with-ggplot2.html", "Section 8 Better plots with ggplot2 8.1 Using ggplot2 properly 8.2 Mapping aesthetics 8.3 geoms: representing the data with different components 8.4 Combining geoms 8.5 Scales and themes: changing the look of your plot 8.6 Other packages use ggplot2 too", " Section 8 Better plots with ggplot2 Rs plotting tools are very good, and in particular, theres a great library called ggplot2 that allows you to put together a huge variety out of plots using a simple system and some common pieces. ggplot2 is loaded whenever you use library(tidyverse), but you can also load it individually: library(ggplot2) cow = carData::Cowles # Recreating the depression variable from before set.seed(1) cow$depression = round( 19 + 0.5 * cow$neuroticism + -0.8 * cow$extraversion + 0.5 * (cow$sex == &quot;female&quot;) + rnorm(nrow(cow), sd = 3) ) 8.1 Using ggplot2 properly ggplot2 will always work best if: Everything you want to show in the plot is in a single dataframe. Each aspect of the plot is represented by a single column. Each column has the right data type, depending on whether its categorical or continuous (factor or numeric). All the data is labelled: categorical variables should already have the labels that you want to show in the plot. You can make plots work if you dont have have those things, but they will be much clunkier to put together. Take the time to put the data into the right shape before you try to produce a plot. 8.2 Mapping aesthetics The most important concept in ggplot2 is mapping each variable in the data to an aesthetic feature of the plot. A wide variety of plots can be put together using a small number of common aesthetics. The aesthetic mapping for a plot is set up using the aes() function. Think about a scatterplot - one variable is mapped to the position along the x-axis, and another is mapped to the position along the y-axis. So a basic scatterplot in ggplot is: ggplot(cow, aes(x = neuroticism, y = depression)) + geom_point() You could also map another variable to the colour of the points: ggplot(cow, aes(x = neuroticism, y = depression, colour = sex)) + geom_point() Sometimes you want to change the look of everything in the plot, without it being specific to individual data points. In that case, you dont put it in the aes() mapping, you set it as an argument for the geom you want to change. Everything that varies based on data should still go in aes(). For example, when scatterplots have lots of points it helps to make them transparent so you can see how many points are overlapping: ggplot(cow, aes(x = neuroticism, y = depression, colour = sex)) + geom_point(alpha = 0.4) 8.3 geoms: representing the data with different components ggplot2 doesnt have functions for specific kinds of plots like a boxplot or bar chart. It does have all the components you need to put these together though. A boxplot will usually have: A categorical variable mapped to x A continuous variable mapped to y Possibly extra variables mapped to the fill colour of the boxes. ggplot(cow, aes(x = sex, y = depression, fill = volunteer)) + geom_boxplot() A bar chart will usually have: A discrete variable mapped to x Another variable mapped to fill that defines the subgroups. No variable mapped to y: y in a bar chart is normally just the count (number of rows) in each subgroup. The bars for the subgroups either stacked on top of each other or side by side (ggplot2 calls this dodging) ggplot(cow, aes(x = sex, fill = volunteer)) + geom_bar(position = &quot;dodge&quot;) Weve done some very standard plot types here to help illustrate how each plot can be broken down into its aesthetic mappings, but the real power of ggplot is that you arent tied to the standard kinds of plots - you can mix and match their components to produce a unique plot that shows exactly what you want. 8.4 Combining geoms Here well produce a plot that shows: The individual data points (jittered a bit so they dont all overlap) Contours that show how dense the points are A line of best fit that shows the overall relationship between two variables When you plot multiple geoms, the order you put them together defines which element will be on top - the first geom is on the bottom. ggplot(cow, aes(x = neuroticism, y = depression)) + stat_density_2d(aes(fill = stat(level)), geom = &quot;polygon&quot;, alpha = 0.6) + geom_jitter(alpha = 0.5) + geom_smooth(colour = &quot;red&quot;) + scale_fill_viridis_c() + theme_bw() This plot is probably a little bit too busy to be really useful - when you have so many options for what you can fit in a plot, you have to make good choices about whats most important to include. 8.5 Scales and themes: changing the look of your plot Once you have a functioning ggplot plot put together, you can start customising the look of it easily by using different scales (which change how a variable maps to a particular aesthetic feature of the plot) and themes (which change more general elements of the plot like the background colour). 8.5.1 Scales Scales serve a few different purposes: Changing the order of values: e.g. reversing the order on the x-axis Transforming values, e.g. log-transforming your y-axis Choosing how your values will be represented - particularly their colours. The scales in ggplot generally have two parts to their names, like scale_x_log10() and scale_colour_gradient(). The two parts indicate: Which aesthetic of the plot the scale is for. What kind of scale will be applied for that aesthetic Scale examples If you just need to reverse the order of a continuous x or y-axis, use scale_x_reverse() or scale_y_reverse(). Returning to our scatter plot from above: ggplot(cow, aes(x = neuroticism, y = depression, colour = sex)) + geom_point(alpha = 0.4) + scale_y_reverse() Log-transforms can be useful for variables like income that are skewed and have a few very large values. Use scale_x_log10() or scale_y_log10(), and note how the axis increases in multiples rather than a constant rate: ggplot(cow, aes(x = sex, fill = volunteer)) + geom_bar(position = &quot;dodge&quot;) + scale_y_log10() Changing the colour scale youre using in your plot is useful both because you can pick a more meaningful scale, and because it can really improve the overall look and clarity of your plot. Picking colours that look good is a valid reason! To demonstrate some of these colour scales, well first create a categorical variable that splits our depression scores up into 5 categories. Well also create a table of the mean neuroticism and extraversion scores in each category: cow$depression_cat = cut( cow$depression, breaks = 5, labels = c(&quot;Very low&quot;, &quot;Low&quot;, &quot;Average&quot;, &quot;High&quot;, &quot;Very high&quot;)) dep_tab = cow %&gt;% group_by(depression_cat) %&gt;% summarize(introversion = mean(neuroticism), extraversion = mean(extraversion)) %&gt;% pivot_longer(-depression_cat, names_to = &quot;Outcome&quot;, values_to = &quot;Mean&quot;) Plotting with the default colours doesnt give great results: dep_cat_plot = ggplot( dep_tab, aes(x = Outcome, y = Mean, fill = depression_cat)) + geom_col(position = &#39;dodge&#39;, colour = &#39;black&#39;) + # \\n creates a newline, i.e. splits the label into multiple lines labs(fill = &quot;Depression\\nCategory&quot;) + theme_bw() dep_cat_plot Since the categories are increasing, we could use a colour scale that maps colours in an ordered way. The viridis colour scales look great and have a few useful properties like converting to greyscale well and being colour-blind friendly. Using an ordered colour scale helps to convey some of the meaning of the different depression categories: dep_cat_plot + # Tweaked slightly to avoid using a very bright yellow # at the top end of the scale scale_fill_viridis_d(option = &quot;C&quot;, end = 0.9) The aesthetic that determines the colour of the bars is fill, not colour, so we use scale_fill to change it. Check whether your plot is using the fill or the colour aesthetic to make sure you change the right one. We could also use a diverging colour scale that emphasises how far each category is from the average. ggplot includes some great scales from ColorBrewer that serve this purpose: dep_cat_plot + scale_fill_brewer(type = &quot;div&quot;) Feel free to experiment with different scales when producing your own plots! 8.5.2 Themes Scales affect how your actual data is displayed, but to control the overall look of the plot (background colour, font etc.), we use themes. To use a different theme, just add it to your plot: dep_cat_plot + scale_fill_viridis_d() + theme_classic() # sjPlot should have been installed during an earlier # session, otherwise install it now dep_cat_plot + scale_fill_viridis_d() + sjPlot::theme_538() 8.5.2.1 Tweaking themes Every aspect of a plots theme can be tweaked using theme(). You can see the full list of properties that can be changed by looking at ?theme. The full details are beyond the scope of this brief introduction, but one useful tweak is turning off particular elements by setting them to element_blank(). We can disable the title on the x-axis: dep_cat_plot + scale_fill_viridis_d() + theme(axis.title.x = element_blank()) 8.5.3 Finding extra scales and themes Packages like ggthemes provide extra scales and themes: install them, load them and you can use the scales and themes just like the built in ones by adding them to your plot. I particularly like ggthemes::theme_fivethirtyeight() and ggthemes::scale_colour_tableau(), but there a wide variety of useful and great-looking options out there. 8.6 Other packages use ggplot2 too "],["real-world-data-example.html", "Section 9 Real world data example 9.1 Data overview 9.2 Basic setup 9.3 Recoding and scoring 9.4 Saving data 9.5 Descriptive statistics 9.6 Analysis", " Section 9 Real world data example In this example, well use some data derived from a real study to get some experience dealing with messy raw data.3 Before we start, well install some packages: install.packages(c(&quot;readxl&quot;, &quot;lme4&quot;)) 9.1 Data overview This data contains the post-treatment observations from a cluster-randomised trial comparing a novel treatment to control. Schools were randomised to different groups, so every participant in the same school received the same intervention. It contains: SURPS scores, a scale assessing different personality traits associated with substance use. The anxiety and depression subscales of the Brief Symptom Inventory Questions about alcohol use The scales havent been scored, so our first step will be scoring them, before we move onto describing and analysing the data. Checking what youve just done is very useful after any change you make to the data. Throughout this example, Ill include notes on how you could check the previous step. Try to code those checks yourself! 9.1.1 SURPS overview The SURPS scale has four subscales: Negative thinking/hopelessness Anxiety sensitivity Impulsivity Sensation seeking Higher scores on each subscale represent higher levels of the personality traits related to substance use, i.e. greater risk. 9.1.2 BSI overview The study includes two subscales from the Brief Symptom Inventory. Higher scores on each represent higher levels of symptoms: Depression Anxiety 9.2 Basic setup 9.2.1 Load libraries As always, the first step is loading the libraries well use in this analysis: library(tidyverse) library(psych) library(sjPlot) library(readxl) library(lme4) # We&#39;ll also set a default theme for our # ggplot plots theme_set(theme_bw()) 9.2.2 Load the data The example data is available online - to download it you can use: download.file(&quot;https://gitlab.com/warsquid/rad/raw/master/data/CapSimulatedData.xlsx&quot;, destfile = &quot;CapSimulatedData.xlsx&quot;, mode = &quot;wb&quot;) (if that doesnt work, please manually download the file here and copy it to your project directory) Now we can load the data in: real = readxl::read_excel(&quot;CapSimulatedData.xlsx&quot;) head(real) ## # A tibble: 6 x 43 ## PersonId SchoolId Sex Group Surps1 Surps2 Surps3 Surps4 Surps5 Surps6 Surps7 Surps8 Surps9 Surps10 Surps11 Surps12 ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 2 Male Control 3 1 1 2 2 2 2 2 1 1 1 1 ## 2 2 7 Female Control NA NA NA NA NA NA NA NA NA NA NA NA ## 3 3 2 Male Treatm~ 2 1 3 2 0 3 2 2 2 3 2 1 ## 4 4 1 Male Control NA NA NA NA NA NA NA NA NA NA NA NA ## 5 5 6 Male Control 3 1 2 2 1 2 2 1 2 1 1 2 ## 6 6 14 Male Treatm~ 3 1 2 3 0 3 3 0 2 0 1 1 ## # ... with 27 more variables: Surps13 &lt;dbl&gt;, Surps14 &lt;dbl&gt;, Surps15 &lt;dbl&gt;, Surps16 &lt;dbl&gt;, Surps17 &lt;dbl&gt;, Surps18 &lt;dbl&gt;, ## # Surps19 &lt;dbl&gt;, Surps20 &lt;dbl&gt;, Surps21 &lt;dbl&gt;, Surps22 &lt;dbl&gt;, Surps23 &lt;dbl&gt;, Bsi1 &lt;dbl&gt;, Bsi2 &lt;dbl&gt;, Bsi3 &lt;dbl&gt;, ## # Bsi4 &lt;dbl&gt;, Bsi5 &lt;dbl&gt;, Bsi6 &lt;dbl&gt;, Bsi7 &lt;dbl&gt;, Bsi8 &lt;dbl&gt;, Bsi9 &lt;dbl&gt;, Bsi10 &lt;dbl&gt;, SipEver &lt;dbl&gt;, FullEver &lt;dbl&gt;, ## # Sip6 &lt;dbl&gt;, Full6 &lt;dbl&gt;, Full6_freq &lt;dbl&gt;, PresentPost &lt;dbl&gt; A good first step is to use str() to inspect the data types: str(real) And check some basic features of the data like the number of people in each intervention group (and maybe school?). table(real$Group) ## ## Control Treatment ## 1001 1189 table(real$SchoolId) ## ## 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 ## 112 114 113 88 126 93 118 125 106 96 108 115 105 96 116 115 108 110 105 121 9.2.3 Solve major issues One major issue with this data is that not everyone completed the post-treatment assessment, and they have missing data for all the questions. In an in-depth analysis we might try to handle this missing data in a more sophisticated way, but for now theres not much we can do with it, so well drop it up front: # Same as: # real = real[real$PresentPost == 1, ] real = filter(real, PresentPost == 1) This is a contrived example, but if you can do something to simplify your data like this, its best to do it up front, before getting into the details of data cleaning and analysis. 9.3 Recoding and scoring 9.3.1 Scoring SURPS the easy way Sometimes there are existing functions in R or in packages that people have written that do exactly what we want with minimal effort. The psych::scoreItems() function is designed for scoring psychometric scales, and it has options to allow a few common tweaks to the scoring process, so it will often score things exactly how you want them. The most useful feature of scoreItems() is it allows you to specify the questions that make up each subscale by providing a list: each element of the list is a character vector specifying which questions are in that subscale. You can put a - in front of a question if that question should be reverse scored. So a simple scale with two subscales might look like: scoring_key = list( Extraversion = c(&quot;Q1&quot;, &quot;-Q2&quot;, &quot;Q3&quot;), Introversion = c(&quot;-Q4&quot;, &quot;-Q5&quot;, &quot;Q6&quot;) ) The SURPS questionnaire has four subscales, and we can score them all at once using: surps_keys = list( Nt = c(&quot;-Surps1&quot;, &quot;-Surps4&quot;, &quot;-Surps7&quot;, &quot;-Surps13&quot;, &quot;Surps17&quot;, &quot;-Surps20&quot;, &quot;-Surps23&quot;), As = c(&quot;Surps8&quot;, &quot;Surps10&quot;, &quot;Surps14&quot;, &quot;Surps18&quot;, &quot;Surps21&quot;), Imp = c(&quot;Surps2&quot;, &quot;Surps5&quot;, &quot;Surps11&quot;, &quot;Surps15&quot;, &quot;Surps22&quot;), Ss = c(&quot;Surps3&quot;, &quot;Surps6&quot;, &quot;Surps9&quot;, &quot;Surps12&quot;, &quot;Surps16&quot;, &quot;Surps19&quot;) ) surps_scored = psych::scoreItems( keys = surps_keys, items = real, totals = TRUE, impute = &quot;median&quot;) We have the scores now, but we havent added them to our main dataset yet. We can look at the object that scoreItems() has given us: surps_scored This is a lot of information, and right now were only interested in the scores, so we need to check how to access them. Looking at the help page ?scoreItems under the Value section tells us the actual scores are stored at surps_scored$scores. We can add all 4 columns into our dataset at once with: real[, c(&quot;NtTotal&quot;, &quot;AsTotal&quot;, &quot;ImpTotal&quot;, &quot;SsTotal&quot;)] = surps_scored$scores head(real[, c(&quot;NtTotal&quot;, &quot;AsTotal&quot;, &quot;ImpTotal&quot;, &quot;SsTotal&quot;)]) ## # A tibble: 6 x 4 ## NtTotal AsTotal ImpTotal SsTotal ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 7 6 7 7 ## 2 6 11 5 12 ## 3 6 6 5 11 ## 4 0 3 2 11 ## 5 8 8 5 12 ## 6 6 1 9 12 Sometimes you dont need all the extra info that psych::scoreItems() provides. psych::scoreFast() will do the same calculations but just return the final scores. 9.3.2 Scoring SURPS the harder way Functions like scoreItems() wont always do exactly what we want. When we scored the scales above, we let scoreItems() impute any missing values using that items median score. However, if we have our own missing data procedure that doesnt match what scoreItems() does, we might have to do some of the work ourselves. One scoring procedure Ive used in the past is: Calculate the scale total for participants who answer all questions Participants that answer fewer than 80% of a scales items get a missing value Participants that answer more than 80% of items get their scores expanded to match the full range based on all items. Some basic math tells us that for those participants who answer &gt;=80% of items, we can calculate the mean of the items they did answer and multiply by the total number of items. To implement our custom procedure, we can do: surps_manual = psych::scoreItems( keys = surps_keys, items = real, totals = FALSE, # Calculate the mean score impute = &quot;none&quot;) real$NtManual = surps_manual$scores[, &quot;Nt&quot;] * length(surps_keys[[&quot;Nt&quot;]]) # Set missing when less than 80% of items scored real$NtManual[surps_manual$missing[, &quot;Nt&quot;] &gt; 1] = NA real$AsManual = surps_manual$scores[, &quot;As&quot;] * length(surps_keys[[&quot;As&quot;]]) real$AsManual[surps_manual$missing[, &quot;As&quot;] &gt; 1] = NA real$ImpManual = surps_manual$scores[, &quot;Imp&quot;] * length(surps_keys[[&quot;Imp&quot;]]) real$ImpManual[surps_manual$missing[, &quot;Imp&quot;] &gt; 1] = NA real$SsManual = surps_manual$scores[, &quot;Ss&quot;] * length(surps_keys[[&quot;Ss&quot;]]) real$SsManual[surps_manual$missing[, &quot;Ss&quot;] &gt; 1] = NA Note how we can use R to calculate some of the numbers involved automatically, like using length(surps_keys[[\"Nt\"]]) instead of typing the actual number. Reusing information that weve already stored can save us from dumb mistakes, since as long as we check that surps_keys has the right items, every piece of code that uses it should also have the right information. How can we figure out the maximum number of missing items for each scale? R can help us with that too: sapply(surps_keys, function(items) { # ceiling() rounds up min_items = ceiling(0.8 * length(items)) max_missing = length(items) - min_items return(max_missing) }) ## Nt As Imp Ss ## 1 1 1 1 As you get more comfortable with R, you can start using it not just to manage your data, but to do some of the extra tasks and calculations that pop up in the process. A simple way to check the scoring would be to look at all the items from one of the subscales along with the total score - calculate a couple of scores manually and see if they match. 9.3.3 Scoring the BSI scales We can score the BSI scales the same way. We just want the total for each subscale, and well assume psychs median imputation is OK: bsi_keys = list( # Using &#39;paste0&#39; to create the column names for us Dep = paste0(&quot;Bsi&quot;, 1:6), Anx = paste0(&quot;Bsi&quot;, 7:10) ) bsi_scored = psych::scoreItems( keys = bsi_keys, items = real, totals = TRUE, impute = &quot;median&quot;) real[, c(&quot;DepTotal&quot;, &quot;AnxTotal&quot;)] = bsi_scored$scores head(real[, c(&quot;DepTotal&quot;, &quot;AnxTotal&quot;)]) ## # A tibble: 6 x 2 ## DepTotal AnxTotal ## &lt;dbl&gt; &lt;dbl&gt; ## 1 4 4 ## 2 1 0 ## 3 0 0 ## 4 0 0 ## 5 1 0 ## 6 1 1 9.3.4 Recoding 9.3.4.1 Recoding categories All the basic recoding tools youd expect are available in R. The most basic tool is converting numeric codes for categorical variables into nicely labelled factors: real$SipEver = factor( real$SipEver, levels = c(0, 1), labels = c(&quot;No&quot;, &quot;Yes&quot;) ) real$FullEver = factor( real$FullEver, levels = c(0, 1), labels = c(&quot;No&quot;, &quot;Yes&quot;) ) real$Sip6 = factor( real$Sip6, levels = c(0, 1), labels = c(&quot;No&quot;, &quot;Yes&quot;) ) real$Full6 = factor( real$Full6, levels = c(0, 1), labels = c(&quot;No&quot;, &quot;Yes&quot;) ) real$Full6_freq = factor( real$Full6_freq, levels = c(0, 1, 2, 3, 4, 5), labels = c(&quot;Never&quot;, &quot;Less than monthly&quot;, &quot;Monthly&quot;, &quot;1-2 times a month&quot;, &quot;Weekly&quot;, &quot;Daily&quot;) ) head(real[, c(&quot;SipEver&quot;, &quot;FullEver&quot;, &quot;Sip6&quot;, &quot;Full6&quot;, &quot;Full6_freq&quot;)]) ## # A tibble: 6 x 5 ## SipEver FullEver Sip6 Full6 Full6_freq ## &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; ## 1 No &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 2 No &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 3 Yes No Yes &lt;NA&gt; &lt;NA&gt; ## 4 Yes No No &lt;NA&gt; &lt;NA&gt; ## 5 Yes No &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 6 Yes No Yes &lt;NA&gt; &lt;NA&gt; Once youve converted the variables to factors, you can combine and reorder categories in different ways using functions from the forcats package (part of the tidyverse). If we want to simplify the frequency variable so its just a Yes/No variable reflecting whether the participant drinks monthly or more often: real$Full6_monthly = fct_collapse( real$Full6_freq, Monthly = c(&quot;Monthly&quot;, &quot;1-2 times a month&quot;, &quot;Weekly&quot;,&quot;Daily&quot;), Less = c(&quot;Never&quot;, &quot;Less than monthly&quot;) ) A good way to check this recoding would be to check the total numbers for each response for each variable. Advanced tip: Avoid repeating yourself In the code above we do the exact same thing repeatedly, just changing the column name each time. R has lots of great tools for applying the same steps multiple times, which become very useful as your data gets larger. The tidyverse has some particularly great tools to make this easier. If we turn the steps for coding a yes/no variable into a function we could apply it to all columns at once, using mutate_at from the dplyr package: # Does the same as the No/Yes recoding above real = mutate_at( real, vars(c(&quot;SipEver&quot;, &quot;FullEver&quot;, &quot;Sip6&quot;, &quot;Full6&quot;)), function(col) { factor(col, levels = c(0, 1), labels = c(&quot;No&quot;, &quot;Yes&quot;)) } ) Scaling/Calculating z-scores The scale() function converts scores to z-scores by mean-centering them and dividing them by their standard deviation. scale() returns a matrix even when we only call it on a single vector, so we need a bit of extra syntax to pull the values out of the matrix: real$NtTotal_z = scale(real$NtTotal)[, 1] real$AsTotal_z = scale(real$AsTotal)[, 1] real$ImpTotal_z = scale(real$ImpTotal)[, 1] real$SsTotal_z = scale(real$SsTotal)[, 1] head(real[, c(&quot;NtTotal&quot;, &quot;NtTotal_z&quot;, &quot;AsTotal&quot;, &quot;AsTotal_z&quot;)]) ## # A tibble: 6 x 4 ## NtTotal NtTotal_z AsTotal AsTotal_z ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 7 0.374 6 -0.346 ## 2 6 0.107 11 1.49 ## 3 6 0.107 6 -0.346 ## 4 0 -1.49 3 -1.45 ## 5 8 0.640 8 0.388 ## 6 6 0.107 1 -2.18 The resulting variables should have means of (very close to) 0 and SDs of 1 - check them by calculating. Advanced tip: reducing repetition Again, we could reduce some of the repetition above using some of the advanced features of the tidyverse. We could do: real = real %&gt;% mutate_at(c(&quot;NtTotal&quot;, &quot;AsTotal&quot;, &quot;ImpTotal&quot;, &quot;SsTotal&quot;), list(z = ~ scale(.)[, 1])) Advanced tip: Scaling within groups If we want to check peoples scores relative to the other participants in their school, then we can scale the scores within each school. Again, this is something thats easiest to handle using the tidyverse: real = real %&gt;% group_by(SchoolId) %&gt;% mutate(NtTotal_schoolz = scale(NtTotal)[, 1], AsTotal_schoolz = scale(AsTotal)[, 1], ImpTotal_schoolz = scale(ImpTotal)[, 1], SsTotal_schoolz = scale(SsTotal)[, 1]) %&gt;% ungroup() head(real[, c(&quot;SchoolId&quot;, &quot;NtTotal&quot;, &quot;NtTotal_z&quot;, &quot;NtTotal_schoolz&quot;)]) ## # A tibble: 6 x 4 ## SchoolId NtTotal NtTotal_z NtTotal_schoolz ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2 7 0.374 0.380 ## 2 2 6 0.107 0.129 ## 3 6 6 0.107 0.157 ## 4 14 0 -1.49 -1.52 ## 5 11 8 0.640 0.683 ## 6 13 6 0.107 0.0594 9.3.4.2 Recoding using logical tests For the BSI scales, well treat any score \\(&gt; 10\\) as showing a possible diagnosis of depression or anxiety: dep_diagnosis = ifelse(real$DepTotal &gt; 10, &quot;Present&quot;, &quot;Absent&quot;) real$DepDiagnosis = factor(dep_diagnosis, levels = c(&quot;Absent&quot;, &quot;Present&quot;)) anx_diagnosis = ifelse(real$AnxTotal &gt; 10, &quot;Present&quot;, &quot;Absent&quot;) real$AnxDiagnosis = factor(anx_diagnosis, levels = c(&quot;Absent&quot;, &quot;Present&quot;)) For SURPS, we classify participants as high risk if they are more than 1 standard deviation above the mean on at least one subscale. First we need to express this as a logical test: is_high_risk = ( (real$NtTotal_z &gt; 1) | (real$AsTotal_z &gt; 1) | (real$ImpTotal_z &gt; 1) | (real$SsTotal_z &gt; 1) ) Then we can convert the logical vector to a factor: real$Risk = factor(is_high_risk, levels = c(FALSE, TRUE), labels = c(&quot;Low&quot;, &quot;High&quot;)) head(real[, c(&quot;NtTotal_z&quot;, &quot;AsTotal_z&quot;, &quot;ImpTotal_z&quot;, &quot;SsTotal_z&quot;, &quot;Risk&quot;)]) ## # A tibble: 6 x 5 ## NtTotal_z AsTotal_z ImpTotal_z SsTotal_z Risk ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 0.374 -0.346 0.195 -0.863 Low ## 2 0.107 1.49 -0.488 0.604 High ## 3 0.107 -0.346 -0.488 0.310 Low ## 4 -1.49 -1.45 -1.51 0.310 Low ## 5 0.640 0.388 -0.488 0.604 Low ## 6 0.107 -2.18 0.877 0.604 Low Relationships between variables: Cleaning up alcohol variables The data here comes from an online survey, which was programmed so that questions are skipped when theyre no longer relevant. So if a participant has never had a sip of alcohol, any questions about having a full drink of alcohol are skipped because we can assume the answer is no. You can see this logic in the plot below, which shows patterns of responses: To get the same kind of overview in table form, you can also use count(): count(real, SipEver, FullEver, Sip6, Full6, Full6_freq) ## # A tibble: 14 x 6 ## SipEver FullEver Sip6 Full6 Full6_freq n ## &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; ## 1 No &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 523 ## 2 Yes No No &lt;NA&gt; &lt;NA&gt; 394 ## 3 Yes No Yes &lt;NA&gt; &lt;NA&gt; 376 ## 4 Yes No &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 16 ## 5 Yes Yes No &lt;NA&gt; &lt;NA&gt; 155 ## 6 Yes Yes Yes No &lt;NA&gt; 11 ## 7 Yes Yes Yes Yes Never 63 ## 8 Yes Yes Yes Yes Less than monthly 11 ## 9 Yes Yes Yes Yes Monthly 10 ## 10 Yes Yes Yes Yes Weekly 1 ## 11 Yes Yes Yes Yes Daily 2 ## 12 Yes Yes &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 4 ## 13 Yes &lt;NA&gt; Yes &lt;NA&gt; &lt;NA&gt; 2 ## 14 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; 50 To analyse the data properly, well need to fill in the No responses that can be assumed (because of the logic of the survey), instead of leaving them missing. Theres no real trick to this, the hard part is getting a clear picture of what needs to be done like we did above. As long as we fill in the variables in order, we should get the right results: # If they&#39;ve never sipped, they&#39;ve never had a full drink # and haven&#39;t sipped in the past 6 months real$FullEver[real$SipEver == &quot;No&quot;] = &quot;No&quot; real$Sip6[real$SipEver == &quot;No&quot;] = &quot;No&quot; # If they haven&#39;t had a full drink ever, they haven&#39;t # had one in the past 6 months real$Full6[real$FullEver == &quot;No&quot;] = &quot;No&quot; # If they haven&#39;t had a sip recently, they haven&#39;t # had a full drink real$Full6[real$Sip6 == &quot;No&quot;] = &quot;No&quot; # If they haven&#39;t had a full drink, their frequency # of drinking is zero real$Full6_freq[real$Full6 == &quot;No&quot;] = &quot;Never&quot; If we look at the pattern of responses again we should see that all the relevant responses have now bene filled in. Where missing values remain, its because we cant automatically assume a response: 9.4 Saving data Now that the data has been recoded and cleaned, we can save it. R can output to lots of different formats, so you can choose whichever format works best for you. However, if you want to keep working in R, its best to save it in a format that preserves all the info about your data, including the order of categories in your factors. SPSS and Stata formats will do this, while Excel wont. For now, well use Rs own .rds format, which preserves all that information: readr::write_rds(real, &quot;CapData-Recoded.rds&quot;) 9.5 Descriptive statistics Once our data has been recoded and cleaned, we can start looking at descriptive statistics - this is always a good first step before trying any actual analysis. 9.5.1 Correlations between variables We can get a good overview of the relationships between different variables using a scatterplot matrix, which shows correlations between each pair of variables. psych has a function pairs.panel to handle this: key_vars = c(&quot;NtTotal_z&quot;, &quot;AsTotal_z&quot;, &quot;ImpTotal_z&quot;, &quot;SsTotal_z&quot;, &quot;DepTotal&quot;, &quot;AnxTotal&quot;) psych::pairs.panels(real[, key_vars]) Also check out the ggcorrplot package for a nicer-looking version of this. If we want a traditional correlation table, we can use psych::corr.test, which provides both the correlations between variables and the p values for each correlation coefficient. psych::corr.test(real[, key_vars]) ## Call:psych::corr.test(x = real[, key_vars]) ## Correlation matrix ## NtTotal_z AsTotal_z ImpTotal_z SsTotal_z DepTotal AnxTotal ## NtTotal_z 1.00 0.10 0.21 -0.13 0.45 0.37 ## AsTotal_z 0.10 1.00 0.12 -0.05 0.11 0.15 ## ImpTotal_z 0.21 0.12 1.00 0.39 0.14 0.14 ## SsTotal_z -0.13 -0.05 0.39 1.00 -0.04 -0.05 ## DepTotal 0.45 0.11 0.14 -0.04 1.00 0.84 ## AnxTotal 0.37 0.15 0.14 -0.05 0.84 1.00 ## Sample Size ## [1] 1618 ## Probability values (Entries above the diagonal are adjusted for multiple tests.) ## NtTotal_z AsTotal_z ImpTotal_z SsTotal_z DepTotal AnxTotal ## NtTotal_z 0 0.00 0 0.00 0.00 0.00 ## AsTotal_z 0 0.00 0 0.08 0.00 0.00 ## ImpTotal_z 0 0.00 0 0.00 0.00 0.00 ## SsTotal_z 0 0.03 0 0.00 0.11 0.11 ## DepTotal 0 0.00 0 0.09 0.00 0.00 ## AnxTotal 0 0.00 0 0.05 0.00 0.00 ## ## To see confidence intervals of the correlations, print with the short=FALSE option 9.5.2 Contingency tables for categorical data To generate cross-tabs or contigency tables for categorical variables, we can use table(), which weve already seen earlier: table(real$Group, real$Sex) ## ## Female Male ## Control 305 423 ## Treatment 386 502 If we want to do some basic testing to see if the proportions of males and females differ, we can use chisq.test() chisq.test(real$Group, real$Sex) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: real$Group and real$Sex ## X-squared = 0.34263, df = 1, p-value = 0.5583 If we wanted to visualize these numbers instead, thats easy to achieve in ggplot: ggplot(real, aes(x = Group, fill = Sex)) + geom_bar(position = &#39;dodge&#39;, colour = &#39;black&#39;) 9.5.3 Understanding complex data Before trying to do analysis, it can be useful to try to understand some of the more complex features of your data. In this study, one important feature is the cluster randomization, where participants are grouped in schools - well need to account for this in our analysis so its worth trying to get a picture of how it looks. Well try to look at the means of some variables in each school. Complex grouped data like this is where the tidyverse starts to be really useful, so well start making heavy use of it. Well start by generating a table: dep_tab = real %&gt;% group_by(Group, SchoolId) %&gt;% summarize( DepMean = mean(DepTotal, na.rm = TRUE), SchoolSize = n() ) head(dep_tab) ## # A tibble: 6 x 4 ## # Groups: Group [1] ## Group SchoolId DepMean SchoolSize ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Control 1 3.03 39 ## 2 Control 2 2.72 40 ## 3 Control 3 2.48 44 ## 4 Control 4 1.17 30 ## 5 Control 5 2.84 32 ## 6 Control 6 2.12 32 Once weve got this information in a table, its easy to create a ggplot plot to visualize it ggplot(dep_tab, aes(x = Group, y = DepMean, colour = Group)) + geom_jitter(aes(size = SchoolSize), alpha= 0.5, width = 0.1, height = 0) You can calculate means and summaries within ggplot2, feeding in your full dataset and using functions like stat_summary(). But Id recommend using tidyverse functions to create a simple summary table instead,as youll often run into things ggplot cant do without a lot of effort. 9.6 Analysis 9.6.1 Simple but wrong: Logistic regression Well start with a simple analysis, just comparing the odds of drinking (a binary outcome) between the groups. Since this is a binary outcome, well use logistic regression, which is available in R through the glm() function. This isnt quite the right approach here, since it doesnt account for potential correlations between participants in the same school. glm() works similarly to lm(), which we saw earlier: we spell out our model using a formula like outcome ~ predictors. For logistic regression we also have to specify family = binomial(link = 'logit'), since thats the distribution were using to model the binary outcomes: simple_glm = glm( Full6 ~ Group, data = real, family = binomial(link = &quot;logit&quot;) ) summary(simple_glm) ## ## Call: ## glm(formula = Full6 ~ Group, family = binomial(link = &quot;logit&quot;), ## data = real) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -0.3595 -0.3595 -0.3197 -0.3197 2.4492 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -2.7066 0.1540 -17.579 &lt;2e-16 *** ## GroupTreatment -0.2416 0.2208 -1.094 0.274 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 671.54 on 1561 degrees of freedom ## Residual deviance: 670.34 on 1560 degrees of freedom ## (56 observations deleted due to missingness) ## AIC: 674.34 ## ## Number of Fisher Scoring iterations: 5 Better output with tab_model() While summary() gives us lots of useful info about the model, its not particularly readable or nice looking. Well use tab_model() for nicer output: tab_model(simple_glm)   Full 6 Predictors Odds Ratios CI p (Intercept) 0.07 0.05  0.09 &lt;0.001 Group [Treatment] 0.79 0.51  1.21 0.274 Observations 1562 R2 Tjur 0.001 Visualizing our model with plot_model() It can be difficult to understand how logistic regression relates to the actual probability of the outcome. Thankfully plot_model() can automatically convert the intervention effect in the model to predicted probabilities: plot_model(simple_glm, type = &quot;pred&quot;, terms = &quot;Group&quot;) 9.6.2 More complex modelling: Mixed models with lme4 To account for the clustering in the data, well use a mixed model from the lme4 package. Adapting our model from above to the mixed model approach doesnt require many changes, since the models use basically the same syntax and we can use some of the same reporting and visualization tools to help interpret them. Instead of the glm() function, well use glmer() from the lme4 package. To add random intercepts for each school into the model, we just need to tweak the syntax slightly: mixed_glm = glmer( Full6 ~ Group + (1 | SchoolId), data = real, family = binomial(link = &quot;logit&quot;) ) summary(mixed_glm) ## Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) [&#39;glmerMod&#39;] ## Family: binomial ( logit ) ## Formula: Full6 ~ Group + (1 | SchoolId) ## Data: real ## ## AIC BIC logLik deviance df.resid ## 676.2 692.3 -335.1 670.2 1559 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -0.2702 -0.2548 -0.2367 -0.2254 4.5686 ## ## Random effects: ## Groups Name Variance Std.Dev. ## SchoolId (Intercept) 0.02679 0.1637 ## Number of obs: 1562, groups: SchoolId, 20 ## ## Fixed effects: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -2.7200 0.1637 -16.616 &lt;2e-16 *** ## GroupTreatment -0.2414 0.2210 -1.092 0.275 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) ## GroupTrtmnt -0.657 9.6.2.1 Using tab_model() again tab_model(mixed_glm)   Full 6 Predictors Odds Ratios CI p (Intercept) 0.07 0.05  0.09 &lt;0.001 Group [Treatment] 0.79 0.51  1.21 0.275 Random Effects 2 3.29 00 SchoolId 0.03 ICC 0.01 N SchoolId 20 Observations 1562 Marginal R2 / Conditional R2 0.004 / 0.012 9.6.2.2 And plot_model() again plot_model(mixed_glm, type = &#39;pred&#39;, terms = &#39;Group&#39;) All the data used here was simulated with synthpop, so it doesnt contain data from any actual participants. "],["learning-moregetting-help.html", "Learning more/Getting help More courses/books Checking out the R community Getting help", " Learning more/Getting help More courses/books If you want more material than is covered in this course, there are lots of different introductions to R out there, many of them free. My number one recommendation is the book R for Data Science, written by Garrett Grolemund and Hadley Wickham (who we met at the start of the course). Its a comprehensive and accessible intro that covers visualization, data wrangling and analysis, all using modern R tools to make things as easy as possible. R for Data Science Other good resources include: swirl, an interactive tutorial for R. The R Graph Gallery: lots of cool examples of different plots. The R Graphics Cookbook, which has a huge number of examples of useful plots, mostly in ggplot2. Any of the other books listed on bookdown.org. The R courses on DataCamp, which have a cool interactive system that tests you as you go. Most are paid but there are free trials to test it out. Checking out the R community If youre on Twitter, check out the #rstats hashtag. Check out R Bloggers, which collects blog posts from a huge number of different R blogs. Getting help If youre stuck with your R code, the best option is to ask someone you know who knows R - some problems can be solved very quickly with a nudge in the right direction from someone whos experienced them before. Stack Overflow is a resource used by programmers the world over. Lots of experienced programmers are happy to answer questions on there to keep their skills sharp and earn useless points (I have a lot of these useless points). The trick to asking a question on Stack Overflow is: Do a few google searches for your question before heading to Stack Overflow. The answer you need might pop up (and will often already be on Stack Overflow). Carefully read the guide to creating a reproducible example for your question. Try to write a question that includes both the code youre already tried, and a copy-pasteable version of your dataset. Stack Overflow works best for small, self-contained coding questions, with an objective answer. If you have a broader question that involves a bit more opinion, you may want to try on the RStudio Community forum. "],["why-r-is-good.html", "Why R is Good R is a Language R is a programming language R has an active community", " Why R is Good Ive tried to keep the actual material in this course focussed on actually teaching R. Here, Ill make a brief attempt at evangelizing for R, and outline why I think its better than other stats-focussed tools like SPSS and Stata. R is a Language R is not just a program that you issue commands to - its a language with consistent rules you can learn, and all the tools in R are written using that language and those rules4. That means: Every tool and function you use is built out of the same basic components. You can write tools and functions that are just as capable and flexible as the built-in tools. If you need to, you can open up other peoples tools and change, modify or improve them. R is a programming language As well as being a language, R is specifically a programming language, and has some of the standard features of programming languages that allow them to be flexible, and to abstract away the low-level details of tasks so you can concentrate on the bigger picture. Every part of your data in R is available as an R object, and you can access, modify and change it the same way as any other data. For example, you can get the column names of your data set as a character vector, which then works the same way as a text column in your actual data. Then you can: Use some code to select a subset of your columns (without typing them all out manually) Write code that will automatically apply the same recoding step to each of those columns (by looping or iterating) over them. Write a function that can carry out these same steps on a brand new dataset with completely different column names. R has an active community Theres a huge wealth of information about R available on the internet, thanks to the fact that people are constantly working on it, discussing it, and making their work public. That means getting help with R is often much easier than in other languages, where it seems like the same 4 academics have been (condescendingly) answering questions for years. The exception to this is that the most basic components of R, like addition of numbers, are written in C for speed. These are usually the components that are so low-level that you dont need to modify them. "],["referencesresources-used.html", "References/Resources used", " References/Resources used "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
