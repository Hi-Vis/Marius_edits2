# Real world data example

In this example, we'll use some data derived from a real study to get
some experience dealing with messy raw data [^simdata].

[^simdata]: All the data used here was simulated with
[synthpop](https://cran.r-project.org/web/packages/synthpop/index.html),
so it doesn't contain data from any actual participants.

Before we start, we'll install some packages:

```{r real_example_packages, eval=FALSE}
install.packages(c("readxl"))
```

## Data overview

This data contains the post-treatment observations from a trial
comparing a novel treatment to control. It contains:

* SURPS scores, a scale assessing different personality traits
  associated with substance use.
* The anxiety and depression subscales of the Brief Symptom Inventory
* Questions about alcohol use

The scales haven't been scored, so our first step will be scoring
them, before we move onto describing and analysing the data. 

```{block, type='note'}
Checking what you've just done is very useful after any
change you make to the data. Throughout this example, I'll
include notes on how you could check the previous step.
Try to code those checks yourself!
```

### SURPS overview

The SURPS scale has four subscales:

* Negative thinking/hopelessness
* Anxiety sensitivity
* Impulsivity
* Sensation seeking

Higher scores on each subscale represent higher levels
of the personality traits related to substance use, i.e.
greater risk.

## Basic setup

### Load libraries

As always, the first step is loading the libraries we'll use in this
analysis:

```{r real_load_libraries, message=FALSE}
library(tidyverse)
library(psych)
library(readxl)
```

### Load the data

The example data is available online - to download it you can use:

```{r download_real_data, eval=FALSE}
download.file("https://gitlab.com/warsquid/rad/raw/master/data/CapSimulatedData.xlsx",
              destfile = "CapSimulatedData.xlsx")
```

Now we can load the data in:

```{r load_real_data, eval=FALSE}
real = readxl::read_excel("CapSimulatedData.xlsx")
head(real)
```

```{r load_real_server, echo=FALSE}
# When building the book/running in development, just
#   load the data from the local folder rather than downloading
#   it
real = readxl::read_excel("data/CapSimulatedData.xlsx")
head(real)
```

A good first step is to use `str()` to inspect the data types:

```{r real_str, results='hide'}
str(real)
```

And check some basic features of the data like the number of people
in each intervention group (and maybe school?).

```{r real_numchecks, options}
table(real$Group)
table(real$SchoolId)
```

### Solve major issues

One major issue with this data is that not everyone completed the
post-treatment assessment, and they have missing data for all
the questions. In an in-depth analysis we might try to handle
this missing data in a more sophisticated way, but for now there's
not much we can do with it, so we'll drop it up front:

```{r drop_nonpresent, options}
# Same as:
# real = real[real$PresentPost == 1, ]
real = filter(real, PresentPost == 1)
```

This is a contrived example, but if you can do something
to simplify your data like this, it's best to do it up front,
before getting into the details of data cleaning and analysis.

## Recoding and scoring

### Scoring SURPS the easy way

Sometimes there are existing functions in R or in packages that people
have written that do exactly what we want with minimal effort.

The `psych::scoreItems()` function is designed for scoring psychometric
scales, and it has options to allow a few common tweaks to the scoring
process, so it will often score things exactly how you want them.

The most useful feature of `scoreItems()` is it allows you to specify the questions
that make up each subscale by providing a **list**: each element
of the list is a **character vector** specifying which questions
are in that subscale. You can put a `-` in front of a question
if that question should be **reverse scored**.

So a simple scale with two subscales might look like:

```{r scoreitems_example, eval=FALSE}
scoring_key = list(
    Extraversion = c("Q1", "-Q2", "Q3"),
    Introversion = c("-Q4", "-Q5", "Q6")
)
```

The SURPS questionnaire has four subscales, and we can score them all at once
using:

```{r psych_simple_surps, options}
surps_keys = list(
    Nt = c("-Surps1", "-Surps4", "-Surps7", "-Surps13", 
           "Surps17", "-Surps20", "-Surps23"),
    As = c("Surps8", "Surps10", "Surps14", "Surps18", 
           "Surps21"),
    Imp = c("Surps2", "Surps5", "Surps11", "Surps15", 
            "Surps22"),
    Ss = c("Surps3", "Surps6", "Surps9", "Surps12", 
               "Surps16", "Surps19")
)

surps_scored = psych::scoreItems(
    keys = surps_keys, 
    items = real,
    totals = TRUE,
    impute = "median")
```

We have the scores now, but we haven't added them to our main dataset yet.
We can look at the object that `scoreItems()` has given us:

```{r psych_inspect_scores, results='hide'}
surps_scored
```

This is a lot of information, and right now we're only interested in the scores,
so we need to check how to access them.

Looking at the help page `?scoreItems` under the **Value** section tells us the 
actual scores are stored at `surps_scored$scores`. We can add all 4 columns into
our dataset at once with:

```{r psych_bind_results, options}
real[, c("NtTotal", "AsTotal", "ImpTotal", "SsTotal")] = surps_scored$scores
head(real[, c("NtTotal", "AsTotal", "ImpTotal", "SsTotal")])
```

```{block, type='note'}
Sometimes you don't need all the extra info that `psych::scoreItems()` provides.
`psych::scoreFast()` will do the same calculations but just return the final
scores.
```

### Scoring SURPS the harder way

Functions like `scoreItems()` won't always do exactly what we want.
When we scored the scales above, we let `scoreItems()` impute any missing
values using that item's median score. However, if we have our own missing
data procedure that doesn't match what `scoreItems()` does, we might have
to do some of the work ourselves.

One scoring procedure I've used in the past is:

* Calculate the scale total for participants who answer all questions
* Participants that answer fewer than 80% of a scale's items get a missing value
* Participants that answer more than 80% of items get their scores "expanded"
  to match the full range based on all items.
  
Some basic math tells us that for those participants who answer >=80% of items,
we can calculate the mean of the items they did answer and multiply by the
total number of items.

To implement our custom procedure, we can do:

```{r psych_scoreitems_custom, options}
surps_manual = psych::scoreItems(
    keys = surps_keys, 
    items = real,
    totals = FALSE, # Calculate the mean score
    impute = "none")

real$NtManual = surps_manual$scores[, "Nt"] * length(surps_keys[["Nt"]])
# Set missing when less than 80% of items scored
real$NtManual[surps_manual$missing[, "Nt"] > 1] = NA

real$AsManual = surps_manual$scores[, "As"] * length(surps_keys[["As"]])
real$AsManual[surps_manual$missing[, "As"] > 1] = NA

real$ImpManual = surps_manual$scores[, "Imp"] * length(surps_keys[["Imp"]])
real$ImpManual[surps_manual$missing[, "Imp"] > 1] = NA

real$SsManual = surps_manual$scores[, "Ss"] * length(surps_keys[["Ss"]])
real$SsManual[surps_manual$missing[, "Ss"] > 1] = NA
```

Note how we can use R to calculate some of the numbers involved automatically,
like using `length(surps_keys[["Nt"]])` instead of typing the actual number.
Reusing information that we've already stored can save us from dumb mistakes,
since as long as we check that `surps_keys` has the right items, every piece
of code that uses it should also have the right information.

How can we figure out the maximum number of missing items for each scale?
R can help us with that too:

```{r calc_missing_nums, options}
sapply(surps_keys, function(items) {
    # ceiling() rounds up
    min_items = ceiling(0.8 * length(items))
    max_missing = length(items) - min_items
    return(max_missing)
})
```

As you get more comfortable with R, you can start using it not just
to manage your data, but to do some of the extra tasks and calculations that
pop up in the process.

```{block, type='note'}
A simple way to check the scoring would be to look at all the items from
one of the subscales along with the total score - calculate
a couple of scores manually and see if they match.
```

### Scoring the BSI scales

We can score the BSI scales the same way. We just want the total
for each subscale, and we'll assume `psych`'s median imputation
is OK:

```{r bsi_scoring_example, options}
bsi_keys = list(
    # Using 'paste0' to create the column names for us
    Dep = paste0("Bsi", 1:6),
    Anx = paste0("Bsi", 7:10)
)

bsi_scored = psych::scoreItems(
    keys = bsi_keys, 
    items = real,
    totals = TRUE,
    impute = "median")

real[, c("DepTotal", "AnxTotal")] = bsi_scored$scores
head(real[, c("DepTotal", "AnxTotal")])
```

### Recoding

#### Scaling/Calculating z-scores

The `scale()` function converts scores to z-scores by mean-centering
them and dividing them by their standard deviation. `scale()` returns
a **matrix** even when we only call it on a single vector, so we need
a bit of extra syntax to pull the values out of the matrix:

```{r scaling_example, options}
real$NtTotal_z = scale(real$NtTotal)[, 1]
real$AsTotal_z = scale(real$AsTotal)[, 1]
real$ImpTotal_z = scale(real$ImpTotal)[, 1]
real$SsTotal_z = scale(real$SsTotal)[, 1]

head(real[, c("NtTotal", "NtTotal_z", "AsTotal", "AsTotal_z")])
```

```{block, type='note'}
The resulting variables should have means of (very close to) 0 and SDs
of 1 - check them by calculating.
```

##### Advanced tip: reducing repetition

If you have to do the same thing multiple times, there's
usually a way to do it automatically, without repeating
yourself. Using some of the advanced features of the [tidyverse](#tidyverse),
we could instead do:

```{r scaling_advanced, eval=FALSE}
real = real %>%
  mutate_at(c("NtTotal", "AsTotal", "ImpTotal", "SsTotal"),
            list(z = ~ scale(.)[, 1]))
```

##### Advanced tip: Scaling within groups

If we want to check peoples' scores relative to the other participants
in their school, then we can scale the scores **within** each school.
Again, this is something that's easiest to handle using the
[tidyverse](#tidyverse):

```{r scale_in_schools, options}
real = real %>%
  group_by(SchoolId) %>%
  mutate(NtTotal_schoolz = scale(NtTotal)[, 1],
         AsTotal_schoolz = scale(AsTotal)[, 1],
         ImpTotal_schoolz = scale(ImpTotal)[, 1],
         SsTotal_schoolz = scale(SsTotal)[, 1]) %>%
  ungroup()

head(real[, c("SchoolId", "NtTotal", "NtTotal_z", "NtTotal_schoolz")])
```

#### Recoding using logical tests

For the BSI scales, we'll treat any score $>= 10$ as showing a possible
diagnosis of depression or anxiety:

```{r bsi_diag_example, options}
dep_diagnosis = ifelse(real$DepTotal > 10, "Present", "Absent")
real$DepDiagnosis = factor(dep_diagnosis, levels = c("Absent", "Present"))

anx_diagnosis = ifelse(real$AnxTotal > 10, "Present", "Absent")
real$AnxDiagnosis = factor(anx_diagnosis, levels = c("Absent", "Present"))
```

For SURPS, we classify participants as **high risk** if 
they are more than 1 standard deviation above the mean on at 
least one subscale. First we need to express this as a logical test:

```{r high_risk_calc, options}
is_high_risk = (
  (real$NtTotal_z > 1) |
  (real$AsTotal_z > 1) |
  (real$ImpTotal_z > 1) |
  (real$SsTotal_z > 1)
)
```

Then we can convert the logical vector to a factor:

```{r high_risk_factor, options}
real$Risk = factor(is_high_risk, levels = c(FALSE, TRUE),
                   labels = c("Low", "High"))

head(real[, c("NtTotal_z", "AsTotal_z", "ImpTotal_z", "SsTotal_z", "Risk")])
```

#### Relationships between variables: Cleaning up alcohol variables

The data here comes from an online survey, which was programmed so that questions
are skipped when they're no longer relevant. So if a participant has never
had a sip of alcohol, any questions about having a full drink of alcohol
are skipped because we can assume the answer is no.

You can see this logic in the plot below, which shows patterns of responses:

```{r alc_logic_plot, echo=FALSE}
real %>%
  ungroup() %>%
  select(PersonId, SipEver, FullEver, Sip6, Full6, Full6_freq) %>%
  mutate_at(vars(-PersonId), ~ factor(.) %>% fct_explicit_na()) %>% 
  count(SipEver, FullEver, Sip6, Full6, Full6_freq) %>%
  mutate(RowId = 1:n()) %>%
  pivot_longer(
    c(-RowId, -n),
    names_to = "Variable",
    values_to = "Response"
  ) %>%
  mutate(
    Variable = fct_inorder(factor(Variable)),
    Response = factor(Response) %>% 
           fct_explicit_na() %>%
           fct_relevel("(Missing)")) %>%
  ggplot(aes(x = Variable, y = RowId, fill = Response)) +
  geom_tile(colour = "black", size=0.4) +
  labs(y = "") +
  scale_y_reverse() +
  scale_fill_manual(values = c("grey60", scales::viridis_pal(option = "B")(5))) +
  theme_minimal() +
  theme(axis.text.y = element_blank(),
        panel.grid = element_blank())
```

To get the same kind of overview in table form, you can also use `count()`:

```{r response_pattern_count, options}
count(real, SipEver, FullEver, Sip6, Full6, Full6_freq)
```

To analyse the data properly, we'll need to fill in the "No" responses that
can be assumed (because of the logic of the survey), instead of leaving
them missing.

There's no real trick to this, the hard part is getting a clear picture
of what needs to be done like we did above.